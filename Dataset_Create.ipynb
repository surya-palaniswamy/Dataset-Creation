{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Dataset_Create.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOnEV4MN6xmX",
        "colab_type": "code",
        "outputId": "1c6f5bc7-b4da-43ee-d7e0-574e52d14aaa",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import cv2 as cv\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import os.path\n",
        "import shutil\n",
        "import random\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wnARVDP6xmj",
        "colab_type": "text"
      },
      "source": [
        "# Location of data and the preliminary details of the model (user entered data) \n",
        "Enter the file path that contains all the assorted images.  \n",
        "Also enter the file location of the CSV file that contains the file name, bounding box details, and the class it belongs to  \n",
        "Enter the size to be resized to "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfnQ6Yt-6xml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "orig_path = ['location/to/image/folder','location/to/annotated/csv/file']\n",
        "train_path = 'location/of/train' \n",
        "test_path = 'location/of/test'\n",
        "size = [32,32,3]\n",
        "split_size = 0.85"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoAQ-Kvy6xmp",
        "colab_type": "text"
      },
      "source": [
        "# All the necessary functions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueDcYojM6xmq",
        "colab_type": "text"
      },
      "source": [
        "## To get all the subfolders of the given dataset folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPpKMYW06xmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def list_files(dir):\n",
        "    r = []\n",
        "    for root, dirs, files in os.walk(dir):\n",
        "        for name in dirs:\n",
        "            r.append(os.path.join(root, name))\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0XAoz3r6xmw",
        "colab_type": "text"
      },
      "source": [
        "## To resize the images to the valid format "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDBpXVeG6xmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_data(dataset,size):\n",
        "    rows,cols,dims = size\n",
        "    dataset = (dataset.resize([rows,cols],Image.BILINEAR))\n",
        "    dataset = list(dataset.getdata())\n",
        "    dataset = np.array(dataset)\n",
        "    dataset = np.resize(dataset,size)\n",
        "    dataset = dataset.reshape(size[0],size[1],size[2])\n",
        "    dataset = dataset.astype('float32')\n",
        "    dataset = dataset/255\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NqdShKg6xm2",
        "colab_type": "text"
      },
      "source": [
        "## Read the images and find the corresponding details of them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24_auMQ96xm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(file_path,size):\n",
        "    target_path = os.path.abspath(file_path)\n",
        "    \n",
        "    img_name = []\n",
        "    data = []\n",
        "    for img in os.listdir(target_path):\n",
        "        scriptpath = os.path.dirname(img)\n",
        "        os.chdir(target_path)\n",
        "        \n",
        "        # Read in the image\n",
        "        image = os.path.join(scriptpath, img)\n",
        "        image_new = Image.open(image)\n",
        "        \n",
        "        #normalise image to our requirements\n",
        "        image = create_data(image_new,size)\n",
        "        img_name.append(img)\n",
        "        data.append(image)\n",
        "    return data,img_name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AKgsbnq6xm7",
        "colab_type": "text"
      },
      "source": [
        "## To relocate the images into folders based on their class and splitting them into test and training set\n",
        "Help in checking for data mismatch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBW8vf4a6xm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relocate(orig,new,details,classes,t_split):\n",
        "    os.chdir(new)\n",
        "    temp = []\n",
        "    train_img = []\n",
        "    train_label = []\n",
        "    test_img = []\n",
        "    test_label = []\n",
        "    create = [new+'train/',new+'test/']\n",
        "    for i in create:\n",
        "        if not os.path.exists(i):\n",
        "            os.mkdir(i)\n",
        "    for i in range(len(classes)):\n",
        "        temp.append(new+str(classes[i]))\n",
        "        ch = new+str(classes[i])\n",
        "        if not os.path.exists(ch):\n",
        "            os.mkdir(ch)\n",
        "    os.chdir(orig)\n",
        "    for i in details:\n",
        "        name = os.path.join(orig,i[0])\n",
        "        for j in temp:\n",
        "            val = j.split('/')\n",
        "            val = val[-1]\n",
        "            if val == i[5]:\n",
        "#                 shutil.move(name,j)   #to move them permenantly\n",
        "                shutil.copy(name,j)   #to copy them \n",
        "    os.chdir(orig)\n",
        "    \n",
        "    \n",
        "# **************************** FOR MAKING TEST AND TRAIN SPLIT MANUALLY************************\n",
        "    for i in temp:\n",
        "        x = [i + '/train',i + '/test']\n",
        "        if not os.path.exists(x[0]):\n",
        "            os.mkdir(x[0])\n",
        "        if not os.path.exists(x[1]):\n",
        "            os.mkdir(x[1])\n",
        "        files = [f for f in os.listdir(i) if os.path.isfile(os.path.join(i, f))]\n",
        "        for j in range(t_split):\n",
        "            shutil.copy(files[j],x[0])\n",
        "        del files[0:t_split]\n",
        "        for j in range(len(files)):\n",
        "            shutil.copy(files[j],x[1])\n",
        "\n",
        "# Flattening out in the below steps is not exactly necessary and can be avoided but no harm done. It is a redundant step            \n",
        "\n",
        "    for i in temp:\n",
        "        a = list_files(i)\n",
        "        for j in a:\n",
        "            b = j.split('/')\n",
        "            b = b[-1]\n",
        "            if b == 'train':\n",
        "                 for img in os.listdir(j):\n",
        "                        scriptpath = os.path.abspath(img)\n",
        "                        new_image = (Image.open(scriptpath))\n",
        "#                         new_image = cv.imread(scriptpath)\n",
        "                        image = create_data(new_image,size)\n",
        "                        train_img.append(image)\n",
        "                        for k in range(len(details)):\n",
        "                            if details[k][0] == img:\n",
        "                                train_label.append([details[k][1:6]])\n",
        "#                                 train_label.append(i.split('_')[-1])\n",
        "            else:\n",
        "                for img in os.listdir(j):\n",
        "                        scriptpath = os.path.abspath(img)\n",
        "                        new_image = (Image.open(scriptpath))\n",
        "#                         new_image = cv.imread(scriptpath)\n",
        "                        image = create_data(new_image,size)\n",
        "                        test_img.append(image)\n",
        "                        for k in range(len(details)):\n",
        "                            if details[k][0] == img:\n",
        "                                test_label.append([details[k][1:6]])\n",
        "#                                 test_label.append(i.split('_')[-1])\n",
        "\n",
        "    for i in temp:\n",
        "        check = list_files(i)\n",
        "        for j in check:\n",
        "            val = j.split('/')\n",
        "            cval = val[-2]\n",
        "            val = val[-1]\n",
        "            if(val == 'train'):\n",
        "                name = os.listdir(j)\n",
        "                new_val = os.path.join(create[0],cval)\n",
        "                if not os.path.exists(new_val):\n",
        "                    os.mkdir(new_val)\n",
        "                for k in name:\n",
        "                    shutil.copy(os.path.join(orig,k),new_val)\n",
        "            else:\n",
        "                name = os.listdir(j)\n",
        "                new_val = os.path.join(create[1],cval)\n",
        "                if not os.path.exists(new_val):\n",
        "                    os.mkdir(new_val)\n",
        "                for k in name:\n",
        "                    shutil.copy(os.path.join(orig,k),new_val)\n",
        "    for i in temp:\n",
        "        shutil.rmtree(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT9VBNZX6xnC",
        "colab_type": "text"
      },
      "source": [
        "## To compile the details of file name, bounding box details, class name of the different Images and then return the training and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dfd7ywd6xnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compile_data(image_dir,annotated_path,size = [64,64,3]):\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import re\n",
        "    data = pd.read_csv(annotated_path)\n",
        "    data_list = data['filename'].tolist()\n",
        "    boundingboxes = []\n",
        "    t_label = []\n",
        "    list_data = []\n",
        "    for i in data['region_shape_attributes']:\n",
        "        boundingboxes.append(re.findall('\\d+', i))\n",
        "        \n",
        "    for j in data['region_attributes']:\n",
        "        temp = j.split(':')\n",
        "        t_label.append((temp[2].split(',')[0])[1:len(temp[2].split(',')[0])-1])\n",
        "    \n",
        "    new = []\n",
        "    for i in range(len(data_list)):\n",
        "        new.append([data_list[i]]+boundingboxes[i])\n",
        "    t_path = image_dir\n",
        "    \n",
        "    t_image = []\n",
        "    t_details = []\n",
        "    img,details = get_data(t_path,size)\n",
        "    for i in range(len(img)):\n",
        "        t_image.append(img[i])\n",
        "        t_details.append(details[i])\n",
        "    for i in range(len(t_image)):\n",
        "        if new[i][0] in t_details:\n",
        "            list_data.append(new[i]+[(t_label[i])])\n",
        "#             print(new[i])\n",
        "    classes = set(t_label)\n",
        "    classes = list(classes)\n",
        "    total_no_img = np.size(t_label)\n",
        "    train_size_per_class = int((total_no_img*split_size)/len(classes))\n",
        "    \n",
        "    relocate(orig_path[0],new_path,list_data,classes,train_size_per_class)\n",
        "    return classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZS_i0vF6xnK",
        "colab_type": "text"
      },
      "source": [
        "## Make the data as CNN readable and convert it to matrix of defined size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHx0meJt6xnN",
        "colab_type": "code",
        "outputId": "1198c096-a19b-4aec-8d49-779701163f37",
        "colab": {}
      },
      "source": [
        "classes = compile_data(orig_path[0],orig_path[1],size)\n",
        "train_batches = ImageDataGenerator().flow_from_directory(train_path, target_size=(224,224), classes=classes, batch_size=3)\n",
        "test_batches = ImageDataGenerator().flow_from_directory(test_path, target_size=(224,224), classes=classes, batch_size=3)\n",
        "print(train_batches)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 12 images belonging to 2 classes.\n",
            "Found 4 images belonging to 2 classes.\n",
            "<keras_preprocessing.image.directory_iterator.DirectoryIterator object at 0x7f1764c48588>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsBRcVHE6xnV",
        "colab_type": "text"
      },
      "source": [
        "## We now save the data into pickle to avoid calculating them again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzE7o0EZ6xnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(new_path)\n",
        "\n",
        "pickle_out = open(\"x_train.pickle\",\"wb\")\n",
        "pickle.dump(x_train, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"y_train.pickle\",\"wb\")\n",
        "pickle.dump(y_train, pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "pickle_out = open(\"z_train.pickle\",\"wb\")\n",
        "pickle.dump(z_train, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}